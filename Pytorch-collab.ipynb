{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36-9Tttj2ozK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Load Data from Google Drive\n",
        "# Set paths to your files on Google Drive (adjust the file names as needed)\n",
        "morskie_oko_mapping_path = '/content/drive/My Drive/MorskieOko_MappingLayers_Stats.csv'\n",
        "summary_stats_path = '/content/drive/My Drive/MorskieOko_summary_stats.csv'\n",
        "yearly_water_area_path = '/content/drive/My Drive/MorskieOko_Yearly_Water_Area.csv'\n",
        "summary_stats_from_geotiffs_path = '/content/drive/My Drive/MorskieOko_Summary_Stats__from_GeoTIFFs_.csv'\n",
        "yearly_water_area_clean_path = '/content/drive/My Drive/Yearly_Water_Area_clean.csv'\n",
        "\n",
        "# Load the CSV files\n",
        "morskie_oko_mapping = pd.read_csv(morskie_oko_mapping_path)\n",
        "summary_stats = pd.read_csv(summary_stats_path)\n",
        "yearly_water_area = pd.read_csv(yearly_water_area_path)\n",
        "summary_stats_from_geotiffs = pd.read_csv(summary_stats_from_geotiffs_path)\n",
        "yearly_water_area_clean = pd.read_csv(yearly_water_area_clean_path)\n",
        "\n",
        "# 3. Merge Dataframes\n",
        "data = pd.merge(morskie_oko_mapping, summary_stats, on='year', how='inner')\n",
        "data = pd.merge(data, yearly_water_area, on='year', how='inner')\n",
        "\n",
        "# Select relevant features for prediction\n",
        "features = data[['occurrence_mean', 'change_abs_mean', 'water_area_mean']]\n",
        "labels = data['pollution_level']  # Assume 'pollution_level' is the target\n",
        "\n",
        "# 4. Data Preprocessing\n",
        "# Handle missing values by filling with the mean\n",
        "features = features.fillna(features.mean())\n",
        "labels = labels.fillna(labels.mean())\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "features = torch.tensor(features, dtype=torch.float32)\n",
        "labels = torch.tensor(labels.values, dtype=torch.float32).view(-1, 1)  # Reshape for regression\n",
        "\n",
        "# 5. Image Preprocessing\n",
        "# Define transformations for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pre-trained models\n",
        "])\n",
        "\n",
        "# 6. Dataset Class for PyTorch\n",
        "class WaterPollutionDataset(Dataset):\n",
        "    def __init__(self, features, labels, image_dir, transform=None):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get tabular features\n",
        "        tabular_data = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Get corresponding image (assuming filenames are indexed by 'idx')\n",
        "        image_path = os.path.join(self.image_dir, f\"{idx + 1}.png\")  # Images are named 1.png, 2.png, etc.\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Apply transformations to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return tabular_data, image, label\n",
        "\n",
        "# Image directory (adjust as needed)\n",
        "image_dir = '/content/drive/My Drive/'  # Make sure the images are in this directory\n",
        "\n",
        "# 7. Split Data into Training and Test Sets\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = WaterPollutionDataset(train_features, train_labels, image_dir, transform)\n",
        "test_dataset = WaterPollutionDataset(test_features, test_labels, image_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 8. Define the Model\n",
        "class WaterPollutionNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaterPollutionNN, self).__init__()\n",
        "\n",
        "        # Image processing (CNN part)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected part for tabular data\n",
        "        self.fc_tabular = nn.Linear(3, 32)  # 3 input features\n",
        "\n",
        "        # Final fully connected layer\n",
        "        self.fc1 = nn.Linear(64*56*56 + 32, 128)  # 64*56*56 is the output of the CNN, plus the tabular features\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output one value (pollution level)\n",
        "\n",
        "    def forward(self, x_tabular, x_image):\n",
        "        # Image processing\n",
        "        x_image = self.pool(torch.relu(self.conv1(x_image)))\n",
        "        x_image = self.pool(torch.relu(self.conv2(x_image)))\n",
        "        x_image = x_image.view(x_image.size(0), -1)  # Flatten\n",
        "\n",
        "        # Tabular processing\n",
        "        x_tabular = torch.relu(self.fc_tabular(x_tabular))\n",
        "\n",
        "        # Concatenate image and tabular features\n",
        "        x = torch.cat((x_image, x_tabular), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = WaterPollutionNN()\n",
        "\n",
        "# 9. Training Setup\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # For regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 10. Training the Model\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs_tabular, inputs_image, targets in train_loader:\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs_tabular, inputs_image)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# 11. Testing the Model\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs_tabular, inputs_image, targets in test_loader:\n",
        "        outputs = model(inputs_tabular, inputs_image)\n",
        "        predictions.append(outputs)\n",
        "        true_labels.append(targets)\n",
        "\n",
        "# Flatten the predictions and true labels for comparison\n",
        "predictions = torch.cat(predictions).numpy()\n",
        "true_labels = torch.cat(true_labels).numpy()\n",
        "\n",
        "# You can print or visualize the predictions and true labels\n",
        "print(\"Predictions: \", predictions)\n",
        "print(\"True Labels: \", true_labels)\n",
        "\n",
        "# 12. Save the Model\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/water_pollution_model_with_images.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hgMYpVoc2rP2",
        "outputId": "5de9eebc-68f5-4726-d776-7a63a19fb68a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1495188089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 1. Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 2. Load Data from Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYALCXKkEy2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T2v91qJHEzLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}